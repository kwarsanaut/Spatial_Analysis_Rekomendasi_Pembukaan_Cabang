{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ab0e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Original data shape: (1587824, 21)\n",
      "Filtered 1276415 rows (active_days <= 22), keeping 311409 rows\n",
      "Processed data shape: (311409, 18)\n",
      "Data saved to: Data_Final_2024_With_Categories_Final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_large_csv(input_file, output_file, kamus_dict, chunk_size=50000):\n",
    "    \"\"\"Process large CSV file in chunks with proper data type handling\"\"\"\n",
    "    \n",
    "    first_chunk = True\n",
    "    processed_rows = 0\n",
    "    filtered_rows = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(input_file, chunksize=chunk_size, dtype=str, keep_default_na=False, na_filter=False):\n",
    "        original_chunk_size = len(chunk)\n",
    "        processed_rows += original_chunk_size\n",
    "        \n",
    "        print(f\"Processing chunk: {processed_rows} rows...\")\n",
    "        \n",
    "        # 1. Add Category column based on dictionary\n",
    "        chunk['Category'] = chunk['brandName'].map(kamus_dict)\n",
    "        chunk['Category'] = chunk['Category'].fillna('Unknown')\n",
    "        \n",
    "        # 2. Convert data types carefully - preserve all original data\n",
    "        chunk = convert_dtypes_preserve(chunk)\n",
    "        \n",
    "        # 3. Filter data - only keep rows with active_days_in_month > 22\n",
    "        if 'active_days_in_month' in chunk.columns:\n",
    "            before_filter = len(chunk)\n",
    "            chunk = chunk[chunk['active_days_in_month'] > 22]\n",
    "            after_filter = len(chunk)\n",
    "            filtered_rows += (before_filter - after_filter)\n",
    "            print(f\"  Filtered {before_filter - after_filter} rows (active_days <= 22), keeping {after_filter} rows\")\n",
    "        \n",
    "        # 4. Save to output file\n",
    "        if first_chunk:\n",
    "            chunk.to_csv(output_file, index=False)\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            chunk.to_csv(output_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    print(f\"Total rows processed: {processed_rows}\")\n",
    "    print(f\"Total rows filtered out (active_days <= 22): {filtered_rows}\")\n",
    "    print(f\"Final dataset size: {processed_rows - filtered_rows}\")\n",
    "\n",
    "def convert_dtypes_preserve(df):\n",
    "    \"\"\"Convert data types while preserving all original data\"\"\"\n",
    "    \n",
    "    # Store original string values for date columns before conversion\n",
    "    if 'sales_year' in df.columns:\n",
    "        df['sales_year_str'] = df['sales_year'].copy()\n",
    "    if 'sales_month' in df.columns:\n",
    "        df['sales_month_str'] = df['sales_month'].copy()\n",
    "    \n",
    "    # Convert ID columns to int64 - handle NaN by filling with 0\n",
    "    id_columns = ['menuID', 'branchID', 'branchCompanyID', 'companyID']\n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            # Convert to numeric, coerce errors to NaN, then fill NaN with 0\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('int64')\n",
    "    \n",
    "    # Convert coordinate columns to float - preserve original null pattern\n",
    "    coord_columns = ['latitude', 'longitude']\n",
    "    for col in coord_columns:\n",
    "        if col in df.columns:\n",
    "            # Convert to numeric, NaN values will remain as NaN (float)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Convert quantity and monetary columns to appropriate types\n",
    "    # For active_days_in_month and total_days_in_month, convert to int\n",
    "    days_columns = ['active_days_in_month', 'total_days_in_month']\n",
    "    for col in days_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('int64')\n",
    "    \n",
    "    # For percentage and monetary values, convert to float\n",
    "    float_columns = [\n",
    "        'total_qty_monthly', 'gtv_monthly', 'aov_monthly', 'active_days_percentage'\n",
    "    ]\n",
    "    \n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            # Clean the data - remove any non-numeric characters except decimal and minus\n",
    "            cleaned = df[col].str.replace(r'[^\\d.-]', '', regex=True)\n",
    "            # Replace empty strings with 0\n",
    "            cleaned = cleaned.replace('', '0')\n",
    "            # Convert to float\n",
    "            df[col] = pd.to_numeric(cleaned, errors='coerce').fillna(0)\n",
    "    \n",
    "    # Convert date columns to numeric but keep string versions\n",
    "    date_columns = ['sales_year', 'sales_month']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('int64')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_final_dataframe_with_correct_dtypes():\n",
    "    \"\"\"Create final dataframe with exact dtypes matching the trial data\"\"\"\n",
    "    \n",
    "    # Read the processed data with specific dtypes for date columns\n",
    "    dtype_spec = {\n",
    "        'sales_year_str': 'object',\n",
    "        'sales_month_str': 'object'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv('Data_Final_2024_With_Categories_Intermediate.csv', \n",
    "                        dtype=dtype_spec)\n",
    "    except:\n",
    "        # If the string columns don't exist, read normally and handle dates differently\n",
    "        df = pd.read_csv('Data_Final_2024_With_Categories_Intermediate.csv')\n",
    "    \n",
    "    # Convert to match trial data types exactly\n",
    "    df = df.rename(columns={\n",
    "        'total_qty_monthly': 'total_qty',\n",
    "        'gtv_monthly': 'gtv_2024', \n",
    "        'aov_monthly': 'aov_2024'\n",
    "    })\n",
    "    \n",
    "    # Create SalesDate column from sales_year and sales_month\n",
    "    # Use string versions if available, otherwise convert numeric to string\n",
    "    if 'sales_year_str' in df.columns and 'sales_month_str' in df.columns:\n",
    "        # Use preserved string versions\n",
    "        df['SalesDate'] = df['sales_year_str'] + '-' + df['sales_month_str'].str.zfill(2) + '-01'\n",
    "    elif 'sales_year' in df.columns and 'sales_month' in df.columns:\n",
    "        # Convert numeric columns to string\n",
    "        df['SalesDate'] = df['sales_year'].astype(str) + '-' + df['sales_month'].astype(str).str.zfill(2) + '-01'\n",
    "    else:\n",
    "        df['SalesDate'] = '2024-01-01'  # Default value\n",
    "    \n",
    "    # Drop temporary string columns if they exist\n",
    "    df = df.drop(columns=['sales_year_str', 'sales_month_str'], errors='ignore')\n",
    "    \n",
    "    # Select and reorder columns to match trial structure\n",
    "    trial_columns = [\n",
    "        'menuID', 'SalesDate', 'cityName', 'brandName', 'branchID', 'branchName',\n",
    "        'branchCode', 'branchCompanyID', 'latitude', 'longitude', 'subdistrictName',\n",
    "        'companyID', 'companyName', 'companyCode', 'total_qty', 'gtv_2024', 'aov_2024', 'Category'\n",
    "    ]\n",
    "    \n",
    "    # Only include columns that exist in our dataframe\n",
    "    available_columns = [col for col in trial_columns if col in df.columns]\n",
    "    df_final = df[available_columns].copy()\n",
    "    \n",
    "    # Ensure final data types match trial exactly\n",
    "    dtype_mapping = {\n",
    "        'menuID': 'int64',\n",
    "        'SalesDate': 'object',\n",
    "        'cityName': 'object', \n",
    "        'brandName': 'object',\n",
    "        'branchID': 'int64',\n",
    "        'branchName': 'object',\n",
    "        'branchCode': 'object',\n",
    "        'branchCompanyID': 'int64', \n",
    "        'latitude': 'float64',\n",
    "        'longitude': 'float64',\n",
    "        'subdistrictName': 'object',\n",
    "        'companyID': 'int64',\n",
    "        'companyName': 'object',\n",
    "        'companyCode': 'object',\n",
    "        'total_qty': 'float64',\n",
    "        'gtv_2024': 'float64',\n",
    "        'aov_2024': 'float64',\n",
    "        'Category': 'object'\n",
    "    }\n",
    "    \n",
    "    for col, dtype in dtype_mapping.items():\n",
    "        if col in df_final.columns:\n",
    "            if dtype == 'int64':\n",
    "                df_final[col] = pd.to_numeric(df_final[col], errors='coerce').fillna(0).astype('int64')\n",
    "            elif dtype == 'float64':\n",
    "                df_final[col] = pd.to_numeric(df_final[col], errors='coerce').astype('float64')\n",
    "            else:\n",
    "                df_final[col] = df_final[col].astype('object')\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def verify_data_consistency(original_file, processed_file):\n",
    "    \"\"\"Verify that no data was lost during processing\"\"\"\n",
    "    \n",
    "    # Read first few rows of original and processed files\n",
    "    original_df = pd.read_csv(original_file, nrows=5, dtype=str)\n",
    "    processed_df = pd.read_csv(processed_file, nrows=5)\n",
    "    \n",
    "    print(\"\\n=== DATA CONSISTENCY CHECK ===\")\n",
    "    print(f\"Original data shape: {original_df.shape}\")\n",
    "    print(f\"Processed data shape: {processed_df.shape}\")\n",
    "    \n",
    "    # Check if all original rows are preserved\n",
    "    original_count = sum(1 for _ in open(original_file)) - 1  # exclude header\n",
    "    processed_count = sum(1 for _ in open(processed_file)) - 1\n",
    "    \n",
    "    print(f\"Original row count: {original_count}\")\n",
    "    print(f\"Processed row count: {processed_count}\")\n",
    "    print(f\"Rows preserved: {original_count == processed_count}\")\n",
    "    \n",
    "    return original_count == processed_count\n",
    "\n",
    "def main():\n",
    "    # 1. Read category dictionary from Excel\n",
    "    print(\"Reading category dictionary...\")\n",
    "    kamus_df = pd.read_excel('Kamus.xlsx')\n",
    "    kamus_dict = dict(zip(kamus_df['Brand Name'], kamus_df['Category']))\n",
    "    print(f\"Loaded {len(kamus_dict)} brand-category mappings\")\n",
    "    \n",
    "    # 2. Process the large CSV file\n",
    "    input_csv = 'Data_Final_2024.csv'\n",
    "    intermediate_output = 'Data_Final_2024_With_Categories_Intermediate.csv'\n",
    "    final_output = 'Data_Final_2024_With_Categories_Final.csv'\n",
    "    \n",
    "    print(\"\\nStarting categorization process...\")\n",
    "    process_large_csv(input_csv, intermediate_output, kamus_dict)\n",
    "    \n",
    "    # 3. Create final dataframe with correct structure\n",
    "    print(\"\\nCreating final dataframe structure...\")\n",
    "    final_df = create_final_dataframe_with_correct_dtypes()\n",
    "    \n",
    "    # 4. Save final result\n",
    "    final_df.to_csv(final_output, index=False)\n",
    "    \n",
    "    # 5. Verification\n",
    "    print(\"\\n=== FINAL VERIFICATION ===\")\n",
    "    consistency_check = verify_data_consistency(input_csv, final_output)\n",
    "    \n",
    "    if consistency_check:\n",
    "        print(\"\\nâœ… SUCCESS: All data preserved during processing!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ WARNING: Some data may have been lost during processing!\")\n",
    "    \n",
    "    # 6. Show final data info\n",
    "    print(f\"\\nFinal dataframe shape: {final_df.shape}\")\n",
    "    print(\"\\nData types:\")\n",
    "    print(final_df.dtypes)\n",
    "    \n",
    "    print(\"\\nNon-null counts:\")\n",
    "    for col in final_df.columns:\n",
    "        non_null = final_df[col].notna().sum()\n",
    "        total = len(final_df)\n",
    "        print(f\"{col}: {non_null}/{total} ({(non_null/total*100):.2f}%)\")\n",
    "    \n",
    "    # Check active_days_in_month distribution in final data\n",
    "    if 'active_days_in_month' in final_df.columns:\n",
    "        active_days_stats = final_df['active_days_in_month'].describe()\n",
    "        print(f\"\\nðŸ“Š Active Days in Month Statistics (After Filtering > 22 days):\")\n",
    "        print(f\"   - Min: {active_days_stats['min']}\")\n",
    "        print(f\"   - Max: {active_days_stats['max']}\")\n",
    "        print(f\"   - Mean: {active_days_stats['mean']:.2f}\")\n",
    "        print(f\"   - Median: {final_df['active_days_in_month'].median()}\")\n",
    "    \n",
    "    # Compare with original\n",
    "    original_df = pd.read_csv(input_csv, dtype=str, nrows=5)\n",
    "    print(f\"\\nOriginal had {len(original_df.columns)} columns, final has {len(final_df.columns)} columns\")\n",
    "    \n",
    "    print(\"\\nSample of categorized data:\")\n",
    "    print(final_df[['brandName', 'Category', 'active_days_in_month']].head(10))\n",
    "\n",
    "# Alternative: Direct processing without intermediate file\n",
    "def process_directly_final():\n",
    "    \"\"\"Process the entire file at once with correct final structure\"\"\"\n",
    "    \n",
    "    # Read category dictionary\n",
    "    kamus_df = pd.read_excel('Kamus.xlsx')\n",
    "    kamus_dict = dict(zip(kamus_df['Brand Name'], kamus_df['Category']))\n",
    "    \n",
    "    # Read the CSV with all columns as string initially\n",
    "    print(\"Reading CSV file...\")\n",
    "    df = pd.read_csv('Data_Final_2024.csv', dtype=str, keep_default_na=False, na_filter=False)\n",
    "    \n",
    "    print(f\"Original data shape: {df.shape}\")\n",
    "    \n",
    "    # Add category column\n",
    "    df['Category'] = df['brandName'].map(kamus_dict)\n",
    "    df['Category'] = df['Category'].fillna('Unknown')\n",
    "    \n",
    "    # Convert data types\n",
    "    df = convert_dtypes_for_final(df)\n",
    "    \n",
    "    # Filter data - only keep rows with active_days_in_month > 22\n",
    "    if 'active_days_in_month' in df.columns:\n",
    "        before_filter = len(df)\n",
    "        df = df[df['active_days_in_month'] > 22]\n",
    "        after_filter = len(df)\n",
    "        print(f\"Filtered {before_filter - after_filter} rows (active_days <= 22), keeping {after_filter} rows\")\n",
    "    \n",
    "    # Create SalesDate column (using string columns directly)\n",
    "    if 'sales_year' in df.columns and 'sales_month' in df.columns:\n",
    "        df['SalesDate'] = df['sales_year'] + '-' + df['sales_month'].str.zfill(2) + '-01'\n",
    "    else:\n",
    "        df['SalesDate'] = '2024-01-01'\n",
    "    \n",
    "    # Rename columns to match trial structure\n",
    "    df = df.rename(columns={\n",
    "        'total_qty_monthly': 'total_qty',\n",
    "        'gtv_monthly': 'gtv_2024', \n",
    "        'aov_monthly': 'aov_2024'\n",
    "    })\n",
    "    \n",
    "    # Select final columns\n",
    "    trial_columns = [\n",
    "        'menuID', 'SalesDate', 'cityName', 'brandName', 'branchID', 'branchName',\n",
    "        'branchCode', 'branchCompanyID', 'latitude', 'longitude', 'subdistrictName',\n",
    "        'companyID', 'companyName', 'companyCode', 'total_qty', 'gtv_2024', 'aov_2024', 'Category'\n",
    "    ]\n",
    "    \n",
    "    available_columns = [col for col in trial_columns if col in df.columns]\n",
    "    df_final = df[available_columns].copy()\n",
    "    \n",
    "    # Convert to final dtypes\n",
    "    df_final = convert_to_final_dtypes(df_final)\n",
    "    \n",
    "    # Save result\n",
    "    output_file = 'Data_Final_2024_With_Categories_Final.csv'\n",
    "    df_final.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Processed data shape: {df_final.shape}\")\n",
    "    print(f\"Data saved to: {output_file}\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def convert_dtypes_for_final(df):\n",
    "    \"\"\"Convert data types for direct processing approach\"\"\"\n",
    "    \n",
    "    # Convert ID columns to int64\n",
    "    id_columns = ['menuID', 'branchID', 'branchCompanyID', 'companyID']\n",
    "    for col in id_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('int64')\n",
    "    \n",
    "    # Convert coordinate columns to float\n",
    "    coord_columns = ['latitude', 'longitude']\n",
    "    for col in coord_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Convert days columns to int\n",
    "    days_columns = ['active_days_in_month', 'total_days_in_month']\n",
    "    for col in days_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('int64')\n",
    "    \n",
    "    # Convert monetary columns to float\n",
    "    float_columns = [\n",
    "        'total_qty_monthly', 'gtv_monthly', 'aov_monthly', 'active_days_percentage'\n",
    "    ]\n",
    "    \n",
    "    for col in float_columns:\n",
    "        if col in df.columns:\n",
    "            cleaned = df[col].str.replace(r'[^\\d.-]', '', regex=True).replace('', '0')\n",
    "            df[col] = pd.to_numeric(cleaned, errors='coerce').fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_to_final_dtypes(df):\n",
    "    \"\"\"Convert to final dtypes matching trial structure\"\"\"\n",
    "    \n",
    "    dtype_mapping = {\n",
    "        'menuID': 'int64',\n",
    "        'SalesDate': 'object',\n",
    "        'cityName': 'object', \n",
    "        'brandName': 'object',\n",
    "        'branchID': 'int64',\n",
    "        'branchName': 'object',\n",
    "        'branchCode': 'object',\n",
    "        'branchCompanyID': 'int64', \n",
    "        'latitude': 'float64',\n",
    "        'longitude': 'float64',\n",
    "        'subdistrictName': 'object',\n",
    "        'companyID': 'int64',\n",
    "        'companyName': 'object',\n",
    "        'companyCode': 'object',\n",
    "        'total_qty': 'float64',\n",
    "        'gtv_2024': 'float64',\n",
    "        'aov_2024': 'float64',\n",
    "        'Category': 'object'\n",
    "    }\n",
    "    \n",
    "    for col, dtype in dtype_mapping.items():\n",
    "        if col in df.columns:\n",
    "            if dtype == 'int64':\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('int64')\n",
    "            elif dtype == 'float64':\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').astype('float64')\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose one method:\n",
    "    \n",
    "    # Method 1: Chunk processing (recommended for very large files)\n",
    "    # main()\n",
    "    \n",
    "    # Method 2: Direct processing (simpler, less error-prone)\n",
    "    df = process_directly_final()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
